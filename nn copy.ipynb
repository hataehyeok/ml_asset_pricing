{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from data_utils import load_info, create_dataloaders, load_preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, output_dim, dropout_prob=0.5):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.LeakyReLU(negative_slope=0.01))\n",
    "            layers.append(nn.Dropout(dropout_prob))\n",
    "            prev_dim = hidden_dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_regularization(model, l1_lambda):\n",
    "    l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "    return l1_lambda * l1_norm\n",
    "\n",
    "def train(model, train_loader, valid_loader, criterion, optimizer, scheduler, epochs, patience, l1_lambda):\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.float(), targets.float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets) + l1_regularization(model, l1_lambda)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in train_loader:\n",
    "                inputs, targets = inputs.float(), targets.float()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        val_loss /= len(valid_loader.dataset)\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    targets_list = []\n",
    "    outputs_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.float(), targets.float()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            targets_list.append(targets.numpy())\n",
    "            outputs_list.append(outputs.numpy())\n",
    "\n",
    "    outputs_list = np.concatenate(outputs_list)\n",
    "    targets_list = np.concatenate(targets_list)\n",
    "\n",
    "    r2 = r2_score(targets_list, outputs_list)\n",
    "    print(f'R² score: {r2}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data, target_data = load_preprocessed_data()\n",
    "firm_info, _ = load_info()\n",
    "train_loader, valid_loader, test_loader, _ = create_dataloaders(\n",
    "    input_data, target_data, firm_info,\n",
    "    train_date='2005-01-01', valid_date='2010-01-01', test_date='2015-11-01', batch_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = input_data.shape[1] - 2\n",
    "hidden_layers_list = [[128, 64, 32], [32, 16, 8], [128, 64], [64, 32], [32, 16], [16, 8]]\n",
    "output_dim = 1\n",
    "learning_rates = [0.05, 0.01]\n",
    "epochs = 100\n",
    "patience = 10\n",
    "l1_lambdas = [1e-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with hidden_layers=[128, 64, 32], learning_rate=0.05, l1_lambda=0.001\n",
      "\n",
      "Epoch 1/100, Train Loss: 0.971981, Val Loss: 0.061118\n",
      "Epoch 2/100, Train Loss: 0.258345, Val Loss: 0.061124\n",
      "Epoch 3/100, Train Loss: 0.256588, Val Loss: 0.061207\n",
      "Epoch 4/100, Train Loss: 0.259445, Val Loss: 0.061303\n",
      "Epoch 5/100, Train Loss: 0.261571, Val Loss: 0.061347\n",
      "Epoch 6/100, Train Loss: 0.263246, Val Loss: 0.061362\n",
      "Epoch 7/100, Train Loss: 0.264492, Val Loss: 0.061346\n",
      "Epoch 8/100, Train Loss: 0.071353, Val Loss: 0.061161\n",
      "Epoch 9/100, Train Loss: 0.053893, Val Loss: 0.061157\n",
      "Epoch 10/100, Train Loss: 0.053972, Val Loss: 0.061152\n",
      "Epoch 11/100, Train Loss: 0.054020, Val Loss: 0.061145\n",
      "Early stopping triggered\n",
      "R² score: -0.01593107048059128\n",
      "\n",
      "\n",
      "Training with hidden_layers=[128, 64, 32], learning_rate=0.01, l1_lambda=0.001\n",
      "\n",
      "Epoch 1/100, Train Loss: 1.438741, Val Loss: 0.060968\n",
      "Epoch 2/100, Train Loss: 0.226698, Val Loss: 0.061142\n",
      "Epoch 3/100, Train Loss: 0.117184, Val Loss: 0.060841\n",
      "Epoch 4/100, Train Loss: 0.093000, Val Loss: 0.061140\n",
      "Epoch 5/100, Train Loss: 0.083477, Val Loss: 0.061806\n",
      "Epoch 6/100, Train Loss: 0.078232, Val Loss: 0.061123\n",
      "Epoch 7/100, Train Loss: 0.076376, Val Loss: 0.061716\n",
      "Epoch 8/100, Train Loss: 0.075025, Val Loss: 0.061098\n",
      "Epoch 9/100, Train Loss: 0.074004, Val Loss: 0.061269\n",
      "Epoch 10/100, Train Loss: 0.040140, Val Loss: 0.061132\n",
      "Epoch 11/100, Train Loss: 0.037063, Val Loss: 0.061225\n",
      "Epoch 12/100, Train Loss: 0.036975, Val Loss: 0.061156\n",
      "Epoch 13/100, Train Loss: 0.036867, Val Loss: 0.061162\n",
      "Early stopping triggered\n",
      "R² score: -0.01750475052665923\n",
      "\n",
      "\n",
      "Training with hidden_layers=[32, 16, 8], learning_rate=0.05, l1_lambda=0.001\n",
      "\n",
      "Epoch 1/100, Train Loss: 0.312803, Val Loss: 0.061098\n",
      "Epoch 2/100, Train Loss: 0.084102, Val Loss: 0.061109\n",
      "Epoch 3/100, Train Loss: 0.075801, Val Loss: 0.061149\n",
      "Epoch 4/100, Train Loss: 0.074255, Val Loss: 0.061208\n",
      "Epoch 5/100, Train Loss: 0.074348, Val Loss: 0.061287\n",
      "Epoch 6/100, Train Loss: 0.074682, Val Loss: 0.061331\n",
      "Epoch 7/100, Train Loss: 0.075109, Val Loss: 0.061361\n",
      "Epoch 8/100, Train Loss: 0.036898, Val Loss: 0.061166\n",
      "Epoch 9/100, Train Loss: 0.033354, Val Loss: 0.061163\n",
      "Epoch 10/100, Train Loss: 0.033382, Val Loss: 0.061161\n",
      "Epoch 11/100, Train Loss: 0.033405, Val Loss: 0.061157\n",
      "Early stopping triggered\n",
      "R² score: -0.017047854672759932\n",
      "\n",
      "\n",
      "Training with hidden_layers=[32, 16, 8], learning_rate=0.01, l1_lambda=0.001\n",
      "\n",
      "Epoch 1/100, Train Loss: 0.501958, Val Loss: 0.061140\n",
      "Epoch 2/100, Train Loss: 0.129553, Val Loss: 0.061109\n",
      "Epoch 3/100, Train Loss: 0.086715, Val Loss: 0.061172\n",
      "Epoch 4/100, Train Loss: 0.067420, Val Loss: 0.061230\n",
      "Epoch 5/100, Train Loss: 0.056510, Val Loss: 0.061201\n",
      "Epoch 6/100, Train Loss: 0.051839, Val Loss: 0.061190\n",
      "Epoch 7/100, Train Loss: 0.047533, Val Loss: 0.061242\n",
      "Epoch 8/100, Train Loss: 0.048233, Val Loss: 0.061194\n",
      "Epoch 9/100, Train Loss: 0.042259, Val Loss: 0.061088\n",
      "Epoch 10/100, Train Loss: 0.039990, Val Loss: 0.061102\n",
      "Epoch 11/100, Train Loss: 0.038938, Val Loss: 0.061135\n",
      "Epoch 12/100, Train Loss: 0.038268, Val Loss: 0.061148\n",
      "Epoch 13/100, Train Loss: 0.037895, Val Loss: 0.061132\n",
      "Epoch 14/100, Train Loss: 0.037630, Val Loss: 0.061125\n",
      "Epoch 15/100, Train Loss: 0.037351, Val Loss: 0.061152\n",
      "Epoch 16/100, Train Loss: 0.036353, Val Loss: 0.061059\n",
      "Epoch 17/100, Train Loss: 0.036238, Val Loss: 0.061033\n",
      "Epoch 18/100, Train Loss: 0.036183, Val Loss: 0.061019\n",
      "Epoch 19/100, Train Loss: 0.036147, Val Loss: 0.061010\n",
      "Epoch 20/100, Train Loss: 0.036130, Val Loss: 0.061036\n",
      "Epoch 21/100, Train Loss: 0.036080, Val Loss: 0.061050\n",
      "Epoch 22/100, Train Loss: 0.036056, Val Loss: 0.061045\n",
      "Epoch 23/100, Train Loss: 0.035992, Val Loss: 0.061014\n",
      "Epoch 24/100, Train Loss: 0.035958, Val Loss: 0.061042\n",
      "Epoch 25/100, Train Loss: 0.035906, Val Loss: 0.061039\n",
      "Epoch 26/100, Train Loss: 0.035772, Val Loss: 0.061012\n",
      "Epoch 27/100, Train Loss: 0.035740, Val Loss: 0.061012\n",
      "Epoch 28/100, Train Loss: 0.035729, Val Loss: 0.061017\n",
      "Epoch 29/100, Train Loss: 0.035722, Val Loss: 0.061014\n",
      "Early stopping triggered\n",
      "R² score: -0.006956391637198633\n",
      "\n",
      "\n",
      "Training with hidden_layers=[128, 64], learning_rate=0.05, l1_lambda=0.001\n",
      "\n",
      "Epoch 1/100, Train Loss: 1.143470, Val Loss: 0.061088\n",
      "Epoch 2/100, Train Loss: 0.230962, Val Loss: 0.061106\n",
      "Epoch 3/100, Train Loss: 0.229622, Val Loss: 0.061118\n",
      "Epoch 4/100, Train Loss: 0.235015, Val Loss: 0.061142\n",
      "Epoch 5/100, Train Loss: 0.239161, Val Loss: 0.061190\n",
      "Epoch 6/100, Train Loss: 0.242281, Val Loss: 0.061255\n",
      "Epoch 7/100, Train Loss: 0.244654, Val Loss: 0.061310\n",
      "Epoch 8/100, Train Loss: 0.068029, Val Loss: 0.061167\n",
      "Epoch 9/100, Train Loss: 0.051913, Val Loss: 0.061166\n",
      "Epoch 10/100, Train Loss: 0.052076, Val Loss: 0.061165\n",
      "Epoch 11/100, Train Loss: 0.052188, Val Loss: 0.061163\n",
      "Early stopping triggered\n",
      "R² score: -0.017606226407004932\n",
      "\n",
      "\n",
      "Training with hidden_layers=[128, 64], learning_rate=0.01, l1_lambda=0.001\n",
      "\n",
      "Epoch 1/100, Train Loss: 1.021558, Val Loss: 0.061045\n",
      "Epoch 2/100, Train Loss: 0.149506, Val Loss: 0.060805\n",
      "Epoch 3/100, Train Loss: 0.092977, Val Loss: 0.061043\n",
      "Epoch 4/100, Train Loss: 0.082686, Val Loss: 0.061111\n",
      "Epoch 5/100, Train Loss: 0.077707, Val Loss: 0.061506\n",
      "Epoch 6/100, Train Loss: 0.074864, Val Loss: 0.061123\n",
      "Epoch 7/100, Train Loss: 0.072751, Val Loss: 0.061418\n",
      "Epoch 8/100, Train Loss: 0.071476, Val Loss: 0.061141\n",
      "Epoch 9/100, Train Loss: 0.037250, Val Loss: 0.061165\n",
      "Epoch 10/100, Train Loss: 0.034168, Val Loss: 0.061166\n",
      "Epoch 11/100, Train Loss: 0.034108, Val Loss: 0.061167\n",
      "Epoch 12/100, Train Loss: 0.034043, Val Loss: 0.061167\n",
      "Early stopping triggered\n",
      "R² score: -0.01795262222509031\n",
      "\n",
      "\n",
      "Training with hidden_layers=[64, 32], learning_rate=0.05, l1_lambda=0.001\n",
      "\n",
      "Epoch 1/100, Train Loss: 0.604318, Val Loss: 0.061382\n",
      "Epoch 2/100, Train Loss: 0.123126, Val Loss: 0.062022\n",
      "Epoch 3/100, Train Loss: 0.121710, Val Loss: 0.061828\n",
      "Epoch 4/100, Train Loss: 0.123059, Val Loss: 0.061535\n",
      "Epoch 5/100, Train Loss: 0.124321, Val Loss: 0.061421\n",
      "Epoch 6/100, Train Loss: 0.125579, Val Loss: 0.061346\n",
      "Epoch 7/100, Train Loss: 0.126585, Val Loss: 0.061352\n",
      "Epoch 8/100, Train Loss: 0.127461, Val Loss: 0.061357\n",
      "Epoch 9/100, Train Loss: 0.128194, Val Loss: 0.061363\n",
      "Epoch 10/100, Train Loss: 0.128758, Val Loss: 0.061405\n",
      "Epoch 11/100, Train Loss: 0.129300, Val Loss: 0.061410\n",
      "Epoch 12/100, Train Loss: 0.129740, Val Loss: 0.061418\n",
      "Epoch 13/100, Train Loss: 0.046566, Val Loss: 0.061098\n",
      "Epoch 14/100, Train Loss: 0.039094, Val Loss: 0.061098\n",
      "Epoch 15/100, Train Loss: 0.039135, Val Loss: 0.061098\n",
      "Epoch 16/100, Train Loss: 0.039164, Val Loss: 0.061099\n",
      "Epoch 17/100, Train Loss: 0.039180, Val Loss: 0.061100\n",
      "Epoch 18/100, Train Loss: 0.039206, Val Loss: 0.061101\n",
      "Epoch 19/100, Train Loss: 0.039235, Val Loss: 0.061102\n",
      "Epoch 20/100, Train Loss: 0.030175, Val Loss: 0.061148\n",
      "Epoch 21/100, Train Loss: 0.029329, Val Loss: 0.061149\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m      8\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml1_lambda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m test(model, test_loader)\n",
      "Cell \u001b[0;32mIn[87], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, valid_loader, criterion, optimizer, scheduler, epochs, patience, l1_lambda)\u001b[0m\n\u001b[1;32m     13\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mfloat(), targets\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets) \u001b[38;5;241m+\u001b[39m l1_regularization(model, l1_lambda)\n\u001b[1;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_asset/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_asset/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[86], line 17\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_asset/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_asset/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_asset/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_asset/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_asset/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_asset/lib/python3.12/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_asset/lib/python3.12/site-packages/torch/nn/functional.py:1295\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for hidden_layers in hidden_layers_list:\n",
    "    for learning_rate in learning_rates:\n",
    "        for l1_lambda in l1_lambdas:\n",
    "            print(f\"Training with hidden_layers={hidden_layers}, learning_rate={learning_rate}, l1_lambda={l1_lambda}\\n\")\n",
    "            model = NeuralNetwork(input_dim, hidden_layers, output_dim, dropout_prob=0.5)\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "            train(model, train_loader, valid_loader, criterion, optimizer, scheduler, epochs, patience, l1_lambda)\n",
    "            test(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_asset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
