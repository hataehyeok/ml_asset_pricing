{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from data_utils import load_info, create_dataloaders, load_preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, output_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.LeakyReLU(negative_slope=0.01))\n",
    "            prev_dim = hidden_dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_regularization(model, l1_lambda):\n",
    "    l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "    return l1_lambda * l1_norm\n",
    "\n",
    "def train(model, train_loader, valid_loader, criterion, optimizer, epochs, patience, l1_lambda):\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.float(), targets.float()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets) + l1_regularization(model, l1_lambda)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in valid_loader:\n",
    "                inputs, targets = inputs.float(), targets.float()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        val_loss /= len(valid_loader.dataset)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}')\n",
    "        \n",
    "        \n",
    "        # if val_loss < best_loss:\n",
    "        #     best_loss = val_loss\n",
    "        #     patience_counter = 0\n",
    "        # else:\n",
    "        #     patience_counter += 1\n",
    "        \n",
    "        # if patience_counter >= patience:\n",
    "        #     print(\"Early stopping triggered\")\n",
    "        #     break\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    targets_list = []\n",
    "    outputs_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.float(), targets.float()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            targets_list.append(targets.numpy())\n",
    "            outputs_list.append(outputs.numpy())\n",
    "\n",
    "    outputs_list = np.concatenate(outputs_list)\n",
    "    targets_list = np.concatenate(targets_list)\n",
    "\n",
    "    r2 = r2_score(targets_list, outputs_list)\n",
    "    print(f'RÂ² score: {r2}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data, target_data = load_preprocessed_data()\n",
    "firm_info, _ = load_info()\n",
    "train_loader, valid_loader, test_loader, _ = create_dataloaders(\n",
    "    input_data, target_data, firm_info,\n",
    "    train_date='2005-01-01', valid_date='2010-01-01', test_date='2015-11-01', batch_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape is(576574, 252)\n",
      "Target data shape: (576574, 3)\n",
      "Train loader length: 114\n",
      "Valid loader length: 52\n",
      "Test loader length: 57\n",
      "Train loader: Batch size = 2000, Features = torch.Size([250])\n",
      "Valid loader: Batch size = 2000, Features = torch.Size([250])\n",
      "Test loader: Batch size = 2000, Features = torch.Size([250])\n"
     ]
    }
   ],
   "source": [
    "print(f'Input data shape is{input_data.shape}')\n",
    "print(f'Target data shape: {target_data.shape}')\n",
    "print(f'Train loader length: {len(train_loader)}')\n",
    "print(f'Valid loader length: {len(valid_loader)}')\n",
    "print(f'Test loader length: {len(test_loader)}')\n",
    "\n",
    "first_batch = next(iter(train_loader))\n",
    "first_batch = next(iter(valid_loader))\n",
    "first_batch = next(iter(test_loader))\n",
    "\n",
    "print(f\"Train loader: Batch size = {len(first_batch[0])}, Features = {first_batch[0][0].shape}\")\n",
    "print(f\"Valid loader: Batch size = {len(first_batch[0])}, Features = {first_batch[0][0].shape}\")\n",
    "print(f\"Test loader: Batch size = {len(first_batch[0])}, Features = {first_batch[0][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters setting\n",
    "input_dim = input_data.shape[1] - 2\n",
    "# hidden_layers_list = [[128, 64, 32], [64, 32, 16], [32, 16, 8], [128, 64], [64, 32], [32, 16], [16, 8], [128], [64], [32], [16], [8]]\n",
    "hidden_layers_list = [[128, 64, 32], [64, 32, 16], [32, 16, 8], [128, 64], [64, 32], [32, 16], [16, 8], ]\n",
    "output_dim = 1\n",
    "learning_rates = [0.01, 0.001]\n",
    "epochs = 100\n",
    "patience = 10\n",
    "l1_lambdas = [1e-5, 1e-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with hidden_layers=[128, 64, 32], learning_rate=0.01, l1_lambda=1e-05\n",
      "\n",
      "Epoch 1/100, Train Loss: 0.034477, Val Loss: 0.026118\n",
      "Epoch 2/100, Train Loss: 0.029705, Val Loss: 0.027166\n",
      "Epoch 3/100, Train Loss: 0.028963, Val Loss: 0.026514\n",
      "Epoch 4/100, Train Loss: 0.028776, Val Loss: 0.026187\n",
      "Epoch 5/100, Train Loss: 0.028656, Val Loss: 0.027865\n",
      "Epoch 6/100, Train Loss: 0.028518, Val Loss: 0.028366\n",
      "Epoch 7/100, Train Loss: 0.028466, Val Loss: 0.028261\n",
      "Epoch 8/100, Train Loss: 0.028499, Val Loss: 0.027362\n",
      "Epoch 9/100, Train Loss: 0.028492, Val Loss: 0.027642\n",
      "Epoch 10/100, Train Loss: 0.028486, Val Loss: 0.027027\n",
      "Epoch 11/100, Train Loss: 0.028406, Val Loss: 0.028156\n",
      "Epoch 12/100, Train Loss: 0.028452, Val Loss: 0.027594\n",
      "Epoch 13/100, Train Loss: 0.028538, Val Loss: 0.028360\n",
      "Epoch 14/100, Train Loss: 0.028458, Val Loss: 0.029835\n",
      "Epoch 15/100, Train Loss: 0.028383, Val Loss: 0.029104\n",
      "Epoch 16/100, Train Loss: 0.028430, Val Loss: 0.028463\n",
      "Epoch 17/100, Train Loss: 0.028450, Val Loss: 0.029054\n",
      "Epoch 18/100, Train Loss: 0.028351, Val Loss: 0.029250\n",
      "Epoch 19/100, Train Loss: 0.028420, Val Loss: 0.029640\n",
      "Epoch 20/100, Train Loss: 0.028496, Val Loss: 0.029962\n",
      "Epoch 21/100, Train Loss: 0.028454, Val Loss: 0.032708\n",
      "Epoch 22/100, Train Loss: 0.028399, Val Loss: 0.030231\n",
      "Epoch 23/100, Train Loss: 0.028463, Val Loss: 0.030158\n",
      "Epoch 24/100, Train Loss: 0.028420, Val Loss: 0.030484\n",
      "Epoch 25/100, Train Loss: 0.028424, Val Loss: 0.030828\n",
      "Epoch 26/100, Train Loss: 0.028429, Val Loss: 0.031185\n",
      "Epoch 27/100, Train Loss: 0.028404, Val Loss: 0.030139\n",
      "Epoch 28/100, Train Loss: 0.028463, Val Loss: 0.031410\n",
      "Epoch 29/100, Train Loss: 0.028402, Val Loss: 0.031079\n",
      "Epoch 30/100, Train Loss: 0.028289, Val Loss: 0.031805\n",
      "Epoch 31/100, Train Loss: 0.028306, Val Loss: 0.033526\n",
      "Epoch 32/100, Train Loss: 0.028368, Val Loss: 0.033255\n",
      "Epoch 33/100, Train Loss: 0.028433, Val Loss: 0.034073\n",
      "Epoch 34/100, Train Loss: 0.028363, Val Loss: 0.033890\n",
      "Epoch 35/100, Train Loss: 0.028417, Val Loss: 0.034843\n",
      "Epoch 36/100, Train Loss: 0.028337, Val Loss: 0.033922\n",
      "Epoch 37/100, Train Loss: 0.028452, Val Loss: 0.030993\n",
      "Epoch 38/100, Train Loss: 0.028657, Val Loss: 0.033046\n",
      "Epoch 39/100, Train Loss: 0.028404, Val Loss: 0.032643\n",
      "Epoch 40/100, Train Loss: 0.028503, Val Loss: 0.036280\n",
      "Epoch 41/100, Train Loss: 0.028336, Val Loss: 0.032974\n",
      "Epoch 42/100, Train Loss: 0.028623, Val Loss: 0.036280\n",
      "Epoch 43/100, Train Loss: 0.028304, Val Loss: 0.033089\n",
      "Epoch 44/100, Train Loss: 0.028600, Val Loss: 0.032175\n",
      "Epoch 45/100, Train Loss: 0.028363, Val Loss: 0.033252\n",
      "Epoch 46/100, Train Loss: 0.028276, Val Loss: 0.034339\n",
      "Epoch 47/100, Train Loss: 0.028406, Val Loss: 0.033070\n",
      "Epoch 48/100, Train Loss: 0.028562, Val Loss: 0.032582\n",
      "Epoch 49/100, Train Loss: 0.028690, Val Loss: 0.031707\n",
      "Epoch 50/100, Train Loss: 0.028507, Val Loss: 0.032174\n",
      "Epoch 51/100, Train Loss: 0.028241, Val Loss: 0.031862\n",
      "Epoch 52/100, Train Loss: 0.028133, Val Loss: 0.033308\n",
      "Epoch 53/100, Train Loss: 0.028089, Val Loss: 0.034114\n",
      "Epoch 54/100, Train Loss: 0.028166, Val Loss: 0.035028\n",
      "Epoch 55/100, Train Loss: 0.028391, Val Loss: 0.032157\n",
      "Epoch 56/100, Train Loss: 0.028697, Val Loss: 0.033832\n",
      "Epoch 57/100, Train Loss: 0.028198, Val Loss: 0.035171\n",
      "Epoch 58/100, Train Loss: 0.028293, Val Loss: 0.033528\n",
      "Epoch 59/100, Train Loss: 0.028382, Val Loss: 0.033344\n",
      "Epoch 60/100, Train Loss: 0.028367, Val Loss: 0.032440\n",
      "Epoch 61/100, Train Loss: 0.028379, Val Loss: 0.033851\n",
      "Epoch 62/100, Train Loss: 0.028221, Val Loss: 0.035383\n",
      "Epoch 63/100, Train Loss: 0.028103, Val Loss: 0.035471\n",
      "Epoch 64/100, Train Loss: 0.028013, Val Loss: 0.036939\n",
      "Epoch 65/100, Train Loss: 0.028088, Val Loss: 0.035265\n",
      "Epoch 66/100, Train Loss: 0.028141, Val Loss: 0.036210\n",
      "Epoch 67/100, Train Loss: 0.028220, Val Loss: 0.035032\n",
      "Epoch 68/100, Train Loss: 0.028371, Val Loss: 0.031271\n",
      "Epoch 69/100, Train Loss: 0.028353, Val Loss: 0.036954\n",
      "Epoch 70/100, Train Loss: 0.028291, Val Loss: 0.032451\n",
      "Epoch 71/100, Train Loss: 0.028568, Val Loss: 0.034953\n",
      "Epoch 72/100, Train Loss: 0.028502, Val Loss: 0.033603\n",
      "Epoch 73/100, Train Loss: 0.028132, Val Loss: 0.037142\n",
      "Epoch 74/100, Train Loss: 0.028065, Val Loss: 0.037544\n",
      "Epoch 75/100, Train Loss: 0.028045, Val Loss: 0.036694\n",
      "Epoch 76/100, Train Loss: 0.028029, Val Loss: 0.036590\n",
      "Epoch 77/100, Train Loss: 0.028182, Val Loss: 0.036900\n",
      "Epoch 78/100, Train Loss: 0.028132, Val Loss: 0.038500\n",
      "Epoch 79/100, Train Loss: 0.028322, Val Loss: 0.035753\n",
      "Epoch 80/100, Train Loss: 0.028478, Val Loss: 0.038306\n",
      "Epoch 81/100, Train Loss: 0.028033, Val Loss: 0.037692\n",
      "Epoch 82/100, Train Loss: 0.027866, Val Loss: 0.038615\n",
      "Epoch 83/100, Train Loss: 0.028245, Val Loss: 0.033558\n",
      "Epoch 84/100, Train Loss: 0.028147, Val Loss: 0.032810\n",
      "Epoch 85/100, Train Loss: 0.028052, Val Loss: 0.042152\n",
      "Epoch 86/100, Train Loss: 0.028017, Val Loss: 0.037148\n",
      "Epoch 87/100, Train Loss: 0.028023, Val Loss: 0.038437\n",
      "Epoch 88/100, Train Loss: 0.028021, Val Loss: 0.040032\n",
      "Epoch 89/100, Train Loss: 0.028256, Val Loss: 0.036575\n",
      "Epoch 90/100, Train Loss: 0.028389, Val Loss: 0.038486\n",
      "Epoch 91/100, Train Loss: 0.028263, Val Loss: 0.040515\n",
      "Epoch 92/100, Train Loss: 0.028118, Val Loss: 0.039414\n",
      "Epoch 93/100, Train Loss: 0.028197, Val Loss: 0.033988\n",
      "Epoch 94/100, Train Loss: 0.028328, Val Loss: 0.039088\n",
      "Epoch 95/100, Train Loss: 0.028180, Val Loss: 0.037849\n",
      "Epoch 96/100, Train Loss: 0.028447, Val Loss: 0.032460\n",
      "Epoch 97/100, Train Loss: 0.028693, Val Loss: 0.040936\n",
      "Epoch 98/100, Train Loss: 0.028282, Val Loss: 0.040162\n",
      "Epoch 99/100, Train Loss: 0.028070, Val Loss: 0.038303\n",
      "Epoch 100/100, Train Loss: 0.028031, Val Loss: 0.039813\n",
      "RÂ² score: -1.4212197573539052\n",
      "\n",
      "\n",
      "Training with hidden_layers=[128, 64, 32], learning_rate=0.01, l1_lambda=0.001\n",
      "\n",
      "Epoch 1/100, Train Loss: 0.160351, Val Loss: 0.025058\n",
      "Epoch 2/100, Train Loss: 0.082534, Val Loss: 0.025110\n",
      "Epoch 3/100, Train Loss: 0.082578, Val Loss: 0.025154\n",
      "Epoch 4/100, Train Loss: 0.082579, Val Loss: 0.025178\n",
      "Epoch 5/100, Train Loss: 0.082585, Val Loss: 0.025191\n",
      "Epoch 6/100, Train Loss: 0.082580, Val Loss: 0.025199\n",
      "Epoch 7/100, Train Loss: 0.082599, Val Loss: 0.025205\n",
      "Epoch 8/100, Train Loss: 0.082612, Val Loss: 0.025206\n",
      "Epoch 9/100, Train Loss: 0.082561, Val Loss: 0.025209\n",
      "Epoch 10/100, Train Loss: 0.082569, Val Loss: 0.025211\n",
      "Epoch 11/100, Train Loss: 0.082622, Val Loss: 0.025213\n",
      "Epoch 12/100, Train Loss: 0.082593, Val Loss: 0.025216\n",
      "Epoch 13/100, Train Loss: 0.082586, Val Loss: 0.025217\n",
      "Epoch 14/100, Train Loss: 0.082565, Val Loss: 0.025218\n",
      "Epoch 15/100, Train Loss: 0.082575, Val Loss: 0.025218\n",
      "Epoch 16/100, Train Loss: 0.082611, Val Loss: 0.025220\n",
      "Epoch 17/100, Train Loss: 0.082565, Val Loss: 0.025220\n",
      "Epoch 18/100, Train Loss: 0.082600, Val Loss: 0.025221\n",
      "Epoch 19/100, Train Loss: 0.082615, Val Loss: 0.025221\n",
      "Epoch 20/100, Train Loss: 0.082555, Val Loss: 0.025221\n",
      "Epoch 21/100, Train Loss: 0.082603, Val Loss: 0.025221\n",
      "Epoch 22/100, Train Loss: 0.082600, Val Loss: 0.025222\n",
      "Epoch 23/100, Train Loss: 0.082605, Val Loss: 0.025221\n",
      "Epoch 24/100, Train Loss: 0.082565, Val Loss: 0.025222\n",
      "Epoch 25/100, Train Loss: 0.082585, Val Loss: 0.025222\n",
      "Epoch 26/100, Train Loss: 0.082599, Val Loss: 0.025223\n",
      "Epoch 27/100, Train Loss: 0.082559, Val Loss: 0.025222\n",
      "Epoch 28/100, Train Loss: 0.082605, Val Loss: 0.025222\n",
      "Epoch 29/100, Train Loss: 0.082581, Val Loss: 0.025222\n",
      "Epoch 30/100, Train Loss: 0.082567, Val Loss: 0.025222\n",
      "Epoch 31/100, Train Loss: 0.082610, Val Loss: 0.025223\n",
      "Epoch 32/100, Train Loss: 0.082575, Val Loss: 0.025222\n",
      "Epoch 33/100, Train Loss: 0.082609, Val Loss: 0.025223\n",
      "Epoch 34/100, Train Loss: 0.082559, Val Loss: 0.025222\n",
      "Epoch 35/100, Train Loss: 0.082588, Val Loss: 0.025222\n",
      "Epoch 36/100, Train Loss: 0.082619, Val Loss: 0.025223\n",
      "Epoch 37/100, Train Loss: 0.082555, Val Loss: 0.025223\n",
      "Epoch 38/100, Train Loss: 0.082587, Val Loss: 0.025223\n",
      "Epoch 39/100, Train Loss: 0.082573, Val Loss: 0.025223\n",
      "Epoch 40/100, Train Loss: 0.082578, Val Loss: 0.025223\n",
      "Epoch 41/100, Train Loss: 0.082579, Val Loss: 0.025223\n",
      "Epoch 42/100, Train Loss: 0.082543, Val Loss: 0.025222\n",
      "Epoch 43/100, Train Loss: 0.082623, Val Loss: 0.025223\n",
      "Epoch 44/100, Train Loss: 0.082562, Val Loss: 0.025223\n",
      "Epoch 45/100, Train Loss: 0.082557, Val Loss: 0.025223\n",
      "Epoch 46/100, Train Loss: 0.082592, Val Loss: 0.025223\n",
      "Epoch 47/100, Train Loss: 0.082595, Val Loss: 0.025223\n",
      "Epoch 48/100, Train Loss: 0.082580, Val Loss: 0.025223\n",
      "Epoch 49/100, Train Loss: 0.082570, Val Loss: 0.025223\n",
      "Epoch 50/100, Train Loss: 0.082585, Val Loss: 0.025223\n",
      "Epoch 51/100, Train Loss: 0.082590, Val Loss: 0.025223\n",
      "Epoch 52/100, Train Loss: 0.082549, Val Loss: 0.025223\n",
      "Epoch 53/100, Train Loss: 0.082606, Val Loss: 0.025223\n",
      "Epoch 54/100, Train Loss: 0.082562, Val Loss: 0.025223\n",
      "Epoch 55/100, Train Loss: 0.082559, Val Loss: 0.025224\n",
      "Epoch 56/100, Train Loss: 0.082596, Val Loss: 0.025223\n",
      "Epoch 57/100, Train Loss: 0.082556, Val Loss: 0.025223\n",
      "Epoch 58/100, Train Loss: 0.082571, Val Loss: 0.025223\n",
      "Epoch 59/100, Train Loss: 0.082582, Val Loss: 0.025223\n",
      "Epoch 60/100, Train Loss: 0.082566, Val Loss: 0.025223\n",
      "Epoch 61/100, Train Loss: 0.082602, Val Loss: 0.025223\n",
      "Epoch 62/100, Train Loss: 0.082553, Val Loss: 0.025223\n",
      "Epoch 63/100, Train Loss: 0.082601, Val Loss: 0.025223\n",
      "Epoch 64/100, Train Loss: 0.082576, Val Loss: 0.025223\n",
      "Epoch 65/100, Train Loss: 0.082554, Val Loss: 0.025223\n",
      "Epoch 66/100, Train Loss: 0.082575, Val Loss: 0.025224\n",
      "Epoch 67/100, Train Loss: 0.082583, Val Loss: 0.025223\n",
      "Epoch 68/100, Train Loss: 0.082590, Val Loss: 0.025223\n",
      "Epoch 69/100, Train Loss: 0.082531, Val Loss: 0.025223\n",
      "Epoch 70/100, Train Loss: 0.082546, Val Loss: 0.025223\n",
      "Epoch 71/100, Train Loss: 0.082606, Val Loss: 0.025223\n",
      "Epoch 72/100, Train Loss: 0.082560, Val Loss: 0.025223\n",
      "Epoch 73/100, Train Loss: 0.082572, Val Loss: 0.025223\n",
      "Epoch 74/100, Train Loss: 0.082565, Val Loss: 0.025223\n",
      "Epoch 75/100, Train Loss: 0.082570, Val Loss: 0.025223\n",
      "Epoch 76/100, Train Loss: 0.082575, Val Loss: 0.025223\n",
      "Epoch 77/100, Train Loss: 0.082564, Val Loss: 0.025223\n",
      "Epoch 78/100, Train Loss: 0.082597, Val Loss: 0.025222\n",
      "Epoch 79/100, Train Loss: 0.082569, Val Loss: 0.025223\n",
      "Epoch 80/100, Train Loss: 0.082546, Val Loss: 0.025223\n",
      "Epoch 81/100, Train Loss: 0.082591, Val Loss: 0.025223\n",
      "Epoch 82/100, Train Loss: 0.082571, Val Loss: 0.025222\n",
      "Epoch 83/100, Train Loss: 0.082579, Val Loss: 0.025223\n",
      "Epoch 84/100, Train Loss: 0.082570, Val Loss: 0.025223\n",
      "Epoch 85/100, Train Loss: 0.082557, Val Loss: 0.025223\n",
      "Epoch 86/100, Train Loss: 0.082572, Val Loss: 0.025222\n",
      "Epoch 87/100, Train Loss: 0.082575, Val Loss: 0.025223\n",
      "Epoch 88/100, Train Loss: 0.082573, Val Loss: 0.025223\n",
      "Epoch 89/100, Train Loss: 0.082585, Val Loss: 0.025223\n",
      "Epoch 90/100, Train Loss: 0.082535, Val Loss: 0.025222\n",
      "Epoch 91/100, Train Loss: 0.082598, Val Loss: 0.025223\n",
      "Epoch 92/100, Train Loss: 0.082579, Val Loss: 0.025223\n",
      "Epoch 93/100, Train Loss: 0.082571, Val Loss: 0.025224\n",
      "Epoch 94/100, Train Loss: 0.082549, Val Loss: 0.025223\n",
      "Epoch 95/100, Train Loss: 0.082564, Val Loss: 0.025223\n",
      "Epoch 96/100, Train Loss: 0.082595, Val Loss: 0.025223\n",
      "Epoch 97/100, Train Loss: 0.082540, Val Loss: 0.025222\n",
      "Epoch 98/100, Train Loss: 0.082563, Val Loss: 0.025223\n",
      "Epoch 99/100, Train Loss: 0.082590, Val Loss: 0.025222\n",
      "Epoch 100/100, Train Loss: 0.082550, Val Loss: 0.025224\n",
      "RÂ² score: -3.720099171800051e-05\n",
      "\n",
      "\n",
      "Training with hidden_layers=[128, 64, 32], learning_rate=0.001, l1_lambda=1e-05\n",
      "\n",
      "Epoch 1/100, Train Loss: 0.039043, Val Loss: 0.026645\n",
      "Epoch 2/100, Train Loss: 0.032398, Val Loss: 0.027226\n",
      "Epoch 3/100, Train Loss: 0.030367, Val Loss: 0.027189\n",
      "Epoch 4/100, Train Loss: 0.029441, Val Loss: 0.027140\n",
      "Epoch 5/100, Train Loss: 0.028935, Val Loss: 0.027103\n",
      "Epoch 6/100, Train Loss: 0.028618, Val Loss: 0.026995\n",
      "Epoch 7/100, Train Loss: 0.028408, Val Loss: 0.027038\n",
      "Epoch 8/100, Train Loss: 0.028268, Val Loss: 0.027247\n",
      "Epoch 9/100, Train Loss: 0.028165, Val Loss: 0.027422\n",
      "Epoch 10/100, Train Loss: 0.028087, Val Loss: 0.027614\n",
      "Epoch 11/100, Train Loss: 0.028023, Val Loss: 0.027823\n",
      "Epoch 12/100, Train Loss: 0.027973, Val Loss: 0.028166\n",
      "Epoch 13/100, Train Loss: 0.027927, Val Loss: 0.028231\n",
      "Epoch 14/100, Train Loss: 0.027881, Val Loss: 0.028549\n",
      "Epoch 15/100, Train Loss: 0.027845, Val Loss: 0.028749\n",
      "Epoch 16/100, Train Loss: 0.027812, Val Loss: 0.028917\n",
      "Epoch 17/100, Train Loss: 0.027780, Val Loss: 0.029077\n",
      "Epoch 18/100, Train Loss: 0.027752, Val Loss: 0.029385\n",
      "Epoch 19/100, Train Loss: 0.027733, Val Loss: 0.029471\n",
      "Epoch 20/100, Train Loss: 0.027714, Val Loss: 0.029523\n",
      "Epoch 21/100, Train Loss: 0.027690, Val Loss: 0.029686\n",
      "Epoch 22/100, Train Loss: 0.027664, Val Loss: 0.029825\n",
      "Epoch 23/100, Train Loss: 0.027645, Val Loss: 0.029993\n",
      "Epoch 24/100, Train Loss: 0.027629, Val Loss: 0.029944\n",
      "Epoch 25/100, Train Loss: 0.027613, Val Loss: 0.030186\n",
      "Epoch 26/100, Train Loss: 0.027601, Val Loss: 0.030089\n",
      "Epoch 27/100, Train Loss: 0.027564, Val Loss: 0.030424\n",
      "Epoch 28/100, Train Loss: 0.027560, Val Loss: 0.030391\n",
      "Epoch 29/100, Train Loss: 0.027540, Val Loss: 0.030478\n",
      "Epoch 30/100, Train Loss: 0.027526, Val Loss: 0.030595\n",
      "Epoch 31/100, Train Loss: 0.027507, Val Loss: 0.030621\n",
      "Epoch 32/100, Train Loss: 0.027519, Val Loss: 0.030942\n",
      "Epoch 33/100, Train Loss: 0.027481, Val Loss: 0.031236\n",
      "Epoch 34/100, Train Loss: 0.027503, Val Loss: 0.031221\n",
      "Epoch 35/100, Train Loss: 0.027460, Val Loss: 0.031268\n",
      "Epoch 36/100, Train Loss: 0.027496, Val Loss: 0.031395\n",
      "Epoch 37/100, Train Loss: 0.027458, Val Loss: 0.031670\n",
      "Epoch 38/100, Train Loss: 0.027406, Val Loss: 0.031619\n",
      "Epoch 39/100, Train Loss: 0.027456, Val Loss: 0.031611\n",
      "Epoch 40/100, Train Loss: 0.027444, Val Loss: 0.031743\n",
      "Epoch 41/100, Train Loss: 0.027398, Val Loss: 0.032262\n",
      "Epoch 42/100, Train Loss: 0.027333, Val Loss: 0.032521\n",
      "Epoch 43/100, Train Loss: 0.027236, Val Loss: 0.032497\n",
      "Epoch 44/100, Train Loss: 0.027298, Val Loss: 0.032469\n",
      "Epoch 45/100, Train Loss: 0.027161, Val Loss: 0.033765\n",
      "Epoch 46/100, Train Loss: 0.027126, Val Loss: 0.032899\n",
      "Epoch 47/100, Train Loss: 0.027181, Val Loss: 0.033215\n",
      "Epoch 48/100, Train Loss: 0.027174, Val Loss: 0.033920\n",
      "Epoch 49/100, Train Loss: 0.027061, Val Loss: 0.035345\n",
      "Epoch 50/100, Train Loss: 0.026993, Val Loss: 0.036584\n",
      "Epoch 51/100, Train Loss: 0.027015, Val Loss: 0.035197\n",
      "Epoch 52/100, Train Loss: 0.026963, Val Loss: 0.036122\n",
      "Epoch 53/100, Train Loss: 0.026890, Val Loss: 0.037323\n",
      "Epoch 54/100, Train Loss: 0.026853, Val Loss: 0.037674\n",
      "Epoch 55/100, Train Loss: 0.026847, Val Loss: 0.037877\n",
      "Epoch 56/100, Train Loss: 0.026821, Val Loss: 0.039210\n",
      "Epoch 57/100, Train Loss: 0.026814, Val Loss: 0.040595\n",
      "Epoch 58/100, Train Loss: 0.026832, Val Loss: 0.040221\n",
      "Epoch 59/100, Train Loss: 0.026834, Val Loss: 0.042175\n",
      "Epoch 60/100, Train Loss: 0.026841, Val Loss: 0.042456\n",
      "Epoch 61/100, Train Loss: 0.026822, Val Loss: 0.042019\n",
      "Epoch 62/100, Train Loss: 0.026829, Val Loss: 0.043468\n",
      "Epoch 63/100, Train Loss: 0.026846, Val Loss: 0.043021\n",
      "Epoch 64/100, Train Loss: 0.026799, Val Loss: 0.042951\n",
      "Epoch 65/100, Train Loss: 0.026765, Val Loss: 0.043840\n",
      "Epoch 66/100, Train Loss: 0.026767, Val Loss: 0.043310\n",
      "Epoch 67/100, Train Loss: 0.026731, Val Loss: 0.044928\n",
      "Epoch 68/100, Train Loss: 0.026722, Val Loss: 0.045563\n",
      "Epoch 69/100, Train Loss: 0.026714, Val Loss: 0.046055\n",
      "Epoch 70/100, Train Loss: 0.026714, Val Loss: 0.046366\n",
      "Epoch 71/100, Train Loss: 0.026707, Val Loss: 0.044874\n",
      "Epoch 72/100, Train Loss: 0.026724, Val Loss: 0.047495\n",
      "Epoch 73/100, Train Loss: 0.026675, Val Loss: 0.047975\n",
      "Epoch 74/100, Train Loss: 0.026675, Val Loss: 0.047156\n",
      "Epoch 75/100, Train Loss: 0.026663, Val Loss: 0.048638\n",
      "Epoch 76/100, Train Loss: 0.026642, Val Loss: 0.048624\n",
      "Epoch 77/100, Train Loss: 0.026654, Val Loss: 0.046630\n",
      "Epoch 78/100, Train Loss: 0.026700, Val Loss: 0.047471\n",
      "Epoch 79/100, Train Loss: 0.026728, Val Loss: 0.046656\n",
      "Epoch 80/100, Train Loss: 0.026631, Val Loss: 0.050541\n",
      "Epoch 81/100, Train Loss: 0.026687, Val Loss: 0.050897\n",
      "Epoch 82/100, Train Loss: 0.026654, Val Loss: 0.051234\n",
      "Epoch 83/100, Train Loss: 0.026614, Val Loss: 0.051727\n",
      "Epoch 84/100, Train Loss: 0.026589, Val Loss: 0.052182\n",
      "Epoch 85/100, Train Loss: 0.026585, Val Loss: 0.050740\n",
      "Epoch 86/100, Train Loss: 0.026545, Val Loss: 0.052300\n",
      "Epoch 87/100, Train Loss: 0.026549, Val Loss: 0.052788\n",
      "Epoch 88/100, Train Loss: 0.026547, Val Loss: 0.052270\n",
      "Epoch 89/100, Train Loss: 0.026568, Val Loss: 0.053255\n",
      "Epoch 90/100, Train Loss: 0.026527, Val Loss: 0.052642\n",
      "Epoch 91/100, Train Loss: 0.026500, Val Loss: 0.052394\n",
      "Epoch 92/100, Train Loss: 0.026481, Val Loss: 0.053098\n",
      "Epoch 93/100, Train Loss: 0.026474, Val Loss: 0.052635\n",
      "Epoch 94/100, Train Loss: 0.026484, Val Loss: 0.054038\n",
      "Epoch 95/100, Train Loss: 0.026465, Val Loss: 0.053938\n",
      "Epoch 96/100, Train Loss: 0.026496, Val Loss: 0.054333\n",
      "Epoch 97/100, Train Loss: 0.026534, Val Loss: 0.052958\n",
      "Epoch 98/100, Train Loss: 0.026490, Val Loss: 0.053092\n",
      "Epoch 99/100, Train Loss: 0.026433, Val Loss: 0.054389\n",
      "Epoch 100/100, Train Loss: 0.026395, Val Loss: 0.054628\n",
      "RÂ² score: -4.427969202071341\n",
      "\n",
      "\n",
      "Training with hidden_layers=[128, 64, 32], learning_rate=0.001, l1_lambda=0.001\n",
      "\n",
      "Epoch 1/100, Train Loss: 0.383446, Val Loss: 0.025004\n",
      "Epoch 2/100, Train Loss: 0.035027, Val Loss: 0.025001\n",
      "Epoch 3/100, Train Loss: 0.034048, Val Loss: 0.025001\n",
      "Epoch 4/100, Train Loss: 0.033531, Val Loss: 0.025003\n",
      "Epoch 5/100, Train Loss: 0.033451, Val Loss: 0.025003\n",
      "Epoch 6/100, Train Loss: 0.033454, Val Loss: 0.025003\n",
      "Epoch 7/100, Train Loss: 0.033453, Val Loss: 0.025003\n",
      "Epoch 8/100, Train Loss: 0.033451, Val Loss: 0.025003\n",
      "Epoch 9/100, Train Loss: 0.033453, Val Loss: 0.025002\n",
      "Epoch 10/100, Train Loss: 0.033452, Val Loss: 0.025002\n",
      "Epoch 11/100, Train Loss: 0.033452, Val Loss: 0.025002\n",
      "Epoch 12/100, Train Loss: 0.033453, Val Loss: 0.025002\n",
      "Epoch 13/100, Train Loss: 0.033452, Val Loss: 0.025002\n",
      "Epoch 14/100, Train Loss: 0.033451, Val Loss: 0.025002\n",
      "Epoch 15/100, Train Loss: 0.033451, Val Loss: 0.025002\n",
      "Epoch 16/100, Train Loss: 0.033453, Val Loss: 0.025002\n",
      "Epoch 17/100, Train Loss: 0.033452, Val Loss: 0.025002\n",
      "Epoch 18/100, Train Loss: 0.033450, Val Loss: 0.025002\n",
      "Epoch 19/100, Train Loss: 0.033452, Val Loss: 0.025002\n",
      "Epoch 20/100, Train Loss: 0.033451, Val Loss: 0.025002\n",
      "Epoch 21/100, Train Loss: 0.033452, Val Loss: 0.025002\n",
      "Epoch 22/100, Train Loss: 0.033453, Val Loss: 0.025002\n",
      "Epoch 23/100, Train Loss: 0.033452, Val Loss: 0.025002\n",
      "Epoch 24/100, Train Loss: 0.033452, Val Loss: 0.025002\n",
      "Epoch 25/100, Train Loss: 0.033452, Val Loss: 0.025002\n",
      "Epoch 26/100, Train Loss: 0.033452, Val Loss: 0.025002\n",
      "Epoch 27/100, Train Loss: 0.033453, Val Loss: 0.025002\n",
      "Epoch 28/100, Train Loss: 0.033452, Val Loss: 0.025002\n",
      "Epoch 29/100, Train Loss: 0.033451, Val Loss: 0.025002\n",
      "Epoch 30/100, Train Loss: 0.033450, Val Loss: 0.025002\n",
      "Epoch 31/100, Train Loss: 0.033452, Val Loss: 0.025002\n",
      "Epoch 32/100, Train Loss: 0.033452, Val Loss: 0.025002\n",
      "Epoch 33/100, Train Loss: 0.033451, Val Loss: 0.025002\n",
      "Epoch 34/100, Train Loss: 0.033453, Val Loss: 0.025002\n",
      "Epoch 35/100, Train Loss: 0.033451, Val Loss: 0.025002\n",
      "Epoch 36/100, Train Loss: 0.033451, Val Loss: 0.025002\n",
      "Epoch 37/100, Train Loss: 0.033452, Val Loss: 0.025002\n",
      "Epoch 38/100, Train Loss: 0.033452, Val Loss: 0.025002\n",
      "Epoch 39/100, Train Loss: 0.033451, Val Loss: 0.025002\n",
      "Epoch 40/100, Train Loss: 0.033451, Val Loss: 0.025002\n",
      "Epoch 41/100, Train Loss: 0.033451, Val Loss: 0.025002\n",
      "Epoch 42/100, Train Loss: 0.033451, Val Loss: 0.025002\n",
      "Epoch 43/100, Train Loss: 0.033450, Val Loss: 0.025002\n",
      "Epoch 44/100, Train Loss: 0.033451, Val Loss: 0.025002\n",
      "Epoch 45/100, Train Loss: 0.033451, Val Loss: 0.025002\n",
      "Epoch 46/100, Train Loss: 0.033450, Val Loss: 0.025002\n",
      "Epoch 47/100, Train Loss: 0.033451, Val Loss: 0.025002\n",
      "Epoch 48/100, Train Loss: 0.033451, Val Loss: 0.025002\n",
      "Epoch 49/100, Train Loss: 0.033451, Val Loss: 0.025002\n",
      "Epoch 50/100, Train Loss: 0.033451, Val Loss: 0.025002\n",
      "Epoch 51/100, Train Loss: 0.033450, Val Loss: 0.025002\n",
      "Epoch 52/100, Train Loss: 0.033452, Val Loss: 0.025002\n",
      "Epoch 53/100, Train Loss: 0.033452, Val Loss: 0.025002\n",
      "Epoch 54/100, Train Loss: 0.033450, Val Loss: 0.025002\n",
      "Epoch 55/100, Train Loss: 0.033452, Val Loss: 0.025002\n",
      "Epoch 56/100, Train Loss: 0.033451, Val Loss: 0.025002\n",
      "Epoch 57/100, Train Loss: 0.033451, Val Loss: 0.025002\n",
      "Epoch 58/100, Train Loss: 0.033449, Val Loss: 0.025002\n",
      "Epoch 59/100, Train Loss: 0.033452, Val Loss: 0.025002\n",
      "Epoch 60/100, Train Loss: 0.033451, Val Loss: 0.025002\n",
      "Epoch 61/100, Train Loss: 0.033450, Val Loss: 0.025002\n",
      "Epoch 62/100, Train Loss: 0.033451, Val Loss: 0.025002\n",
      "Epoch 63/100, Train Loss: 0.033450, Val Loss: 0.025002\n",
      "Epoch 64/100, Train Loss: 0.033450, Val Loss: 0.025002\n",
      "Epoch 65/100, Train Loss: 0.033452, Val Loss: 0.025002\n",
      "Epoch 66/100, Train Loss: 0.033451, Val Loss: 0.025002\n",
      "Epoch 67/100, Train Loss: 0.033450, Val Loss: 0.025002\n",
      "Epoch 68/100, Train Loss: 0.033450, Val Loss: 0.025002\n",
      "Epoch 69/100, Train Loss: 0.033451, Val Loss: 0.025002\n",
      "Epoch 70/100, Train Loss: 0.033449, Val Loss: 0.025002\n",
      "Epoch 71/100, Train Loss: 0.033450, Val Loss: 0.025002\n",
      "Epoch 72/100, Train Loss: 0.033450, Val Loss: 0.025002\n",
      "Epoch 73/100, Train Loss: 0.033450, Val Loss: 0.025002\n",
      "Epoch 74/100, Train Loss: 0.033449, Val Loss: 0.025002\n",
      "Epoch 75/100, Train Loss: 0.033450, Val Loss: 0.025002\n",
      "Epoch 76/100, Train Loss: 0.033451, Val Loss: 0.025002\n",
      "Epoch 77/100, Train Loss: 0.033450, Val Loss: 0.025002\n",
      "Epoch 78/100, Train Loss: 0.033449, Val Loss: 0.025002\n",
      "Epoch 79/100, Train Loss: 0.033450, Val Loss: 0.025002\n",
      "Epoch 80/100, Train Loss: 0.033451, Val Loss: 0.025002\n",
      "Epoch 81/100, Train Loss: 0.033450, Val Loss: 0.025002\n",
      "Epoch 82/100, Train Loss: 0.033450, Val Loss: 0.025002\n",
      "Epoch 83/100, Train Loss: 0.033450, Val Loss: 0.025002\n",
      "Epoch 84/100, Train Loss: 0.033449, Val Loss: 0.025002\n",
      "Epoch 85/100, Train Loss: 0.033449, Val Loss: 0.025002\n",
      "Epoch 86/100, Train Loss: 0.033450, Val Loss: 0.025002\n",
      "Epoch 87/100, Train Loss: 0.033450, Val Loss: 0.025002\n",
      "Epoch 88/100, Train Loss: 0.033448, Val Loss: 0.025002\n",
      "Epoch 89/100, Train Loss: 0.033449, Val Loss: 0.025002\n",
      "Epoch 90/100, Train Loss: 0.033450, Val Loss: 0.025002\n",
      "Epoch 91/100, Train Loss: 0.033450, Val Loss: 0.025002\n",
      "Epoch 92/100, Train Loss: 0.033449, Val Loss: 0.025002\n",
      "Epoch 93/100, Train Loss: 0.033449, Val Loss: 0.025002\n",
      "Epoch 94/100, Train Loss: 0.033449, Val Loss: 0.025002\n",
      "Epoch 95/100, Train Loss: 0.033449, Val Loss: 0.025002\n",
      "Epoch 96/100, Train Loss: 0.033450, Val Loss: 0.025002\n",
      "Epoch 97/100, Train Loss: 0.033449, Val Loss: 0.025002\n",
      "Epoch 98/100, Train Loss: 0.033448, Val Loss: 0.025002\n",
      "Epoch 99/100, Train Loss: 0.033449, Val Loss: 0.025002\n",
      "Epoch 100/100, Train Loss: 0.033449, Val Loss: 0.025002\n",
      "RÂ² score: -0.01785644085638438\n",
      "\n",
      "\n",
      "Training with hidden_layers=[64, 32, 16], learning_rate=0.01, l1_lambda=1e-05\n",
      "\n",
      "Epoch 1/100, Train Loss: 0.032694, Val Loss: 0.025977\n",
      "Epoch 2/100, Train Loss: 0.029442, Val Loss: 0.026212\n",
      "Epoch 3/100, Train Loss: 0.028916, Val Loss: 0.026227\n",
      "Epoch 4/100, Train Loss: 0.028725, Val Loss: 0.026143\n",
      "Epoch 5/100, Train Loss: 0.028582, Val Loss: 0.026215\n",
      "Epoch 6/100, Train Loss: 0.028466, Val Loss: 0.026083\n",
      "Epoch 7/100, Train Loss: 0.028370, Val Loss: 0.025904\n",
      "Epoch 8/100, Train Loss: 0.028264, Val Loss: 0.026191\n",
      "Epoch 9/100, Train Loss: 0.028289, Val Loss: 0.026198\n",
      "Epoch 10/100, Train Loss: 0.028233, Val Loss: 0.026034\n",
      "Epoch 11/100, Train Loss: 0.028248, Val Loss: 0.026321\n",
      "Epoch 12/100, Train Loss: 0.028218, Val Loss: 0.026562\n",
      "Epoch 13/100, Train Loss: 0.028170, Val Loss: 0.026952\n",
      "Epoch 14/100, Train Loss: 0.028243, Val Loss: 0.026272\n",
      "Epoch 15/100, Train Loss: 0.028209, Val Loss: 0.026564\n",
      "Epoch 16/100, Train Loss: 0.028188, Val Loss: 0.026502\n",
      "Epoch 17/100, Train Loss: 0.028206, Val Loss: 0.026877\n",
      "Epoch 18/100, Train Loss: 0.028179, Val Loss: 0.027334\n",
      "Epoch 19/100, Train Loss: 0.028188, Val Loss: 0.026454\n",
      "Epoch 20/100, Train Loss: 0.028074, Val Loss: 0.026560\n",
      "Epoch 21/100, Train Loss: 0.028204, Val Loss: 0.026655\n",
      "Epoch 22/100, Train Loss: 0.028134, Val Loss: 0.026753\n",
      "Epoch 23/100, Train Loss: 0.028173, Val Loss: 0.026841\n",
      "Epoch 24/100, Train Loss: 0.028192, Val Loss: 0.026632\n",
      "Epoch 25/100, Train Loss: 0.028180, Val Loss: 0.026930\n",
      "Epoch 26/100, Train Loss: 0.028214, Val Loss: 0.026816\n",
      "Epoch 27/100, Train Loss: 0.028187, Val Loss: 0.027080\n",
      "Epoch 28/100, Train Loss: 0.028333, Val Loss: 0.026831\n",
      "Epoch 29/100, Train Loss: 0.028170, Val Loss: 0.026974\n",
      "Epoch 30/100, Train Loss: 0.028140, Val Loss: 0.026822\n",
      "Epoch 31/100, Train Loss: 0.028132, Val Loss: 0.026738\n",
      "Epoch 32/100, Train Loss: 0.028238, Val Loss: 0.026965\n",
      "Epoch 33/100, Train Loss: 0.028197, Val Loss: 0.027036\n",
      "Epoch 34/100, Train Loss: 0.028228, Val Loss: 0.026871\n",
      "Epoch 35/100, Train Loss: 0.028098, Val Loss: 0.027142\n",
      "Epoch 36/100, Train Loss: 0.028274, Val Loss: 0.026645\n",
      "Epoch 37/100, Train Loss: 0.028091, Val Loss: 0.026954\n",
      "Epoch 38/100, Train Loss: 0.028186, Val Loss: 0.026847\n",
      "Epoch 39/100, Train Loss: 0.028145, Val Loss: 0.027132\n",
      "Epoch 40/100, Train Loss: 0.028198, Val Loss: 0.026797\n",
      "Epoch 41/100, Train Loss: 0.028203, Val Loss: 0.027111\n",
      "Epoch 42/100, Train Loss: 0.028040, Val Loss: 0.026912\n",
      "Epoch 43/100, Train Loss: 0.028103, Val Loss: 0.026901\n",
      "Epoch 44/100, Train Loss: 0.028088, Val Loss: 0.027373\n",
      "Epoch 45/100, Train Loss: 0.028208, Val Loss: 0.027031\n",
      "Epoch 46/100, Train Loss: 0.028134, Val Loss: 0.027260\n",
      "Epoch 47/100, Train Loss: 0.028110, Val Loss: 0.027481\n",
      "Epoch 48/100, Train Loss: 0.028146, Val Loss: 0.027598\n",
      "Epoch 49/100, Train Loss: 0.028219, Val Loss: 0.027781\n",
      "Epoch 50/100, Train Loss: 0.028221, Val Loss: 0.028080\n",
      "Epoch 51/100, Train Loss: 0.028230, Val Loss: 0.027305\n",
      "Epoch 52/100, Train Loss: 0.028218, Val Loss: 0.027291\n",
      "Epoch 53/100, Train Loss: 0.028169, Val Loss: 0.027071\n",
      "Epoch 54/100, Train Loss: 0.028213, Val Loss: 0.027008\n",
      "Epoch 55/100, Train Loss: 0.028198, Val Loss: 0.027301\n",
      "Epoch 56/100, Train Loss: 0.028148, Val Loss: 0.027687\n",
      "Epoch 57/100, Train Loss: 0.028170, Val Loss: 0.027734\n",
      "Epoch 58/100, Train Loss: 0.028120, Val Loss: 0.027104\n",
      "Epoch 59/100, Train Loss: 0.028115, Val Loss: 0.027825\n",
      "Epoch 60/100, Train Loss: 0.028102, Val Loss: 0.027671\n",
      "Epoch 61/100, Train Loss: 0.028181, Val Loss: 0.026620\n",
      "Epoch 62/100, Train Loss: 0.028183, Val Loss: 0.027502\n",
      "Epoch 63/100, Train Loss: 0.027969, Val Loss: 0.027108\n",
      "Epoch 64/100, Train Loss: 0.028121, Val Loss: 0.027689\n",
      "Epoch 65/100, Train Loss: 0.028166, Val Loss: 0.026720\n",
      "Epoch 66/100, Train Loss: 0.027921, Val Loss: 0.027619\n",
      "Epoch 67/100, Train Loss: 0.028134, Val Loss: 0.027560\n",
      "Epoch 68/100, Train Loss: 0.028142, Val Loss: 0.027349\n",
      "Epoch 69/100, Train Loss: 0.027965, Val Loss: 0.027661\n",
      "Epoch 70/100, Train Loss: 0.028154, Val Loss: 0.026962\n",
      "Epoch 71/100, Train Loss: 0.028408, Val Loss: 0.027056\n",
      "Epoch 72/100, Train Loss: 0.027979, Val Loss: 0.027588\n",
      "Epoch 73/100, Train Loss: 0.027891, Val Loss: 0.026985\n",
      "Epoch 74/100, Train Loss: 0.028130, Val Loss: 0.027234\n",
      "Epoch 75/100, Train Loss: 0.027946, Val Loss: 0.027200\n",
      "Epoch 76/100, Train Loss: 0.028294, Val Loss: 0.027665\n",
      "Epoch 77/100, Train Loss: 0.028465, Val Loss: 0.025688\n",
      "Epoch 78/100, Train Loss: 0.029263, Val Loss: 0.025051\n",
      "Epoch 79/100, Train Loss: 0.028125, Val Loss: 0.028554\n",
      "Epoch 80/100, Train Loss: 0.027982, Val Loss: 0.025991\n",
      "Epoch 81/100, Train Loss: 0.029439, Val Loss: 0.025562\n",
      "Epoch 82/100, Train Loss: 0.028789, Val Loss: 0.025954\n",
      "Epoch 83/100, Train Loss: 0.028791, Val Loss: 0.025373\n",
      "Epoch 84/100, Train Loss: 0.028601, Val Loss: 0.026524\n",
      "Epoch 85/100, Train Loss: 0.028550, Val Loss: 0.026301\n",
      "Epoch 86/100, Train Loss: 0.028523, Val Loss: 0.026801\n",
      "Epoch 87/100, Train Loss: 0.028437, Val Loss: 0.026567\n",
      "Epoch 88/100, Train Loss: 0.028424, Val Loss: 0.026548\n",
      "Epoch 89/100, Train Loss: 0.028370, Val Loss: 0.026572\n",
      "Epoch 90/100, Train Loss: 0.028338, Val Loss: 0.027413\n",
      "Epoch 91/100, Train Loss: 0.028240, Val Loss: 0.027043\n",
      "Epoch 92/100, Train Loss: 0.028182, Val Loss: 0.027620\n",
      "Epoch 93/100, Train Loss: 0.028077, Val Loss: 0.026825\n",
      "Epoch 94/100, Train Loss: 0.028117, Val Loss: 0.026974\n",
      "Epoch 95/100, Train Loss: 0.028101, Val Loss: 0.027405\n",
      "Epoch 96/100, Train Loss: 0.028088, Val Loss: 0.027566\n",
      "Epoch 97/100, Train Loss: 0.027979, Val Loss: 0.027579\n",
      "Epoch 98/100, Train Loss: 0.028123, Val Loss: 0.027545\n",
      "Epoch 99/100, Train Loss: 0.028035, Val Loss: 0.027764\n",
      "Epoch 100/100, Train Loss: 0.028226, Val Loss: 0.027427\n",
      "RÂ² score: -0.058379568463408305\n",
      "\n",
      "\n",
      "Training with hidden_layers=[64, 32, 16], learning_rate=0.01, l1_lambda=0.001\n",
      "\n",
      "Epoch 1/100, Train Loss: 0.103360, Val Loss: 0.025022\n",
      "Epoch 2/100, Train Loss: 0.051782, Val Loss: 0.025025\n",
      "Epoch 3/100, Train Loss: 0.051334, Val Loss: 0.025027\n",
      "Epoch 4/100, Train Loss: 0.051103, Val Loss: 0.025025\n",
      "Epoch 5/100, Train Loss: 0.051023, Val Loss: 0.025034\n",
      "Epoch 6/100, Train Loss: 0.051032, Val Loss: 0.025041\n",
      "Epoch 7/100, Train Loss: 0.051047, Val Loss: 0.025047\n",
      "Epoch 8/100, Train Loss: 0.051071, Val Loss: 0.025053\n"
     ]
    }
   ],
   "source": [
    "for hidden_layers in hidden_layers_list:\n",
    "    for learning_rate in learning_rates:\n",
    "        for l1_lambda in l1_lambdas:\n",
    "            print(f\"Training with hidden_layers={hidden_layers}, learning_rate={learning_rate}, l1_lambda={l1_lambda}\\n\")\n",
    "            model = NeuralNetwork(input_dim, hidden_layers, output_dim)\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "            train(model, train_loader, valid_loader, criterion, optimizer, epochs, patience, l1_lambda)\n",
    "            test(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_asset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
